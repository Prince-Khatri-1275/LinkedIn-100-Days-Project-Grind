{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6f9d07e",
   "metadata": {},
   "source": [
    "Hide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfd0042",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__():\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4054fc",
   "metadata": {},
   "source": [
    "Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4c3f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.keras.datasets.mnist import load_data as load_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "682cd2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, d_in, d_out):\n",
    "        \"\"\"\n",
    "            We are using He Initialization for the weights and biases for this Linear Layer\n",
    "        \"\"\"\n",
    "        \n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        \n",
    "        self.W = np.random.randn(d_in, d_out) * (2/np.sqrt(d_in))\n",
    "        self.B = np.zeros(d_out)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        return inputs @ self.W + self.B\n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "        self.dW = self.inputs.T @ dvalues\n",
    "        self.dB = np.sum(d_values, axis=range(len(dvalues.shape)-1))\n",
    "        \n",
    "        return dvalues @ self.W.T\n",
    "    \n",
    "    def __call__(self, X):\n",
    "        return self.forward(X)\n",
    "    \n",
    "    def __get_info__(self):\n",
    "        return {\"name\": \"Linear\", \"params\": (self.d_in, self.d_out), \"additional_info\": \"This is a linear layer initialized with He-Initialization for Weights W and Biases B\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ac64b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation:\n",
    "    def __init__(self, func, grad, name=\"father\", additional_info=None):\n",
    "        self.func = func\n",
    "        self.grad = grad\n",
    "        \n",
    "        self.name = name\n",
    "        self.inputs = None\n",
    "        \n",
    "        self.additional_info = additional_info\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        return self.func(inputs)\n",
    "    \n",
    "    def backward(self, d_values):\n",
    "        return self.grad(inputs) * d_values\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "    \n",
    "    def __get_info__(self):\n",
    "        return {\"name\": str(self.name)+\"Activation\", \"params\": self.inputs.shape if self.inputs else None, \"additional_info\":f\"This is the main Activation Class use to mainly introduce non-linearity to the model.\" if self.additional_info is not None else self.additional_info}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3416ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Activation):\n",
    "    def __init__(self):\n",
    "        func = lambda x: np.maximum(x, 0)\n",
    "        grad = lambda x: np.where(x>0, 1, 0)\n",
    "        super().__init__(func, grad, name=\"ReLU\", additional_info=\"It is Rectified Linear Units introduces non-linearity although not in a smooth way but in a manner introducing less compute.\")\n",
    "        \n",
    "class Sigmoid(Activation):\n",
    "    def __init__(self):\n",
    "        func = lambda x: 1/(1+np.exp(-x))\n",
    "        grad = lambda x: self._helper(func(x))\n",
    "        super().__init__(func, grad, name=\"ReLU\", additional_info=\"It is Activation Function that uses sigmoid function to introduce non-linearity. By the way its range is (0, 1)\")\n",
    "        \n",
    "    def _helper(self, x):\n",
    "        return x - x**2\n",
    "    \n",
    "class TanH(Activation):\n",
    "    def __init__(self):\n",
    "        func = lambda x: np.tanh(x)\n",
    "        grad = lambda x: 1-func(x)**2\n",
    "        super().__init__(func, grad, name=\"ReLU\", additional_info=\"It uses tan hyperbolic function thus introduces non-linearity it is in the range of (-1, 1)\")\n",
    "\n",
    "class Sigmoid(Activation):\n",
    "    def __init__(self, loss_c=True):\n",
    "        func = lambda x: self._helper_func\n",
    "        grad = lambda x: self._helper_grad(x, loss_c=loss_c)\n",
    "        super().__init__(func, grad, name=\"ReLU\", additional_info=\"It is Activation Function that uses sigmoid function to introduce non-linearity. By the way its range is (0, 1)\")\n",
    "        \n",
    "    def _helper_func(self, x):\n",
    "        mx_val = np.max(x, axis=-1, keepdims=True)\n",
    "        exp_vals = np.exp(x-mx_val)\n",
    "        \n",
    "        return exp_vals / np.sum(x, axis=-1, keepdims=True)\n",
    "    \n",
    "    def _helper_grad(self, x, loss_c):\n",
    "        if loss_c:\n",
    "            return 1\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
